{
  "total_time_consume": 6442.225262403488,
  "preprocess": {
    "total_time": 0.135300874710083
  },
  "classifier": [
    {
      "ins_id": 0,
      "dataset_partition": {
        "bulid_graph_time": 15.445606470108032,
        "save_graph_time": 11.039048194885254,
        "graph_partition_time": 43.520262479782104,
        "total_time": 70.06662964820862,
        "cluster_number_distribution": [
          1604,
          1580,
          1605,
          1476,
          1555,
          1605,
          1508,
          1600,
          1406,
          1605,
          1370,
          1513,
          1605,
          1592,
          1600,
          1605,
          1579,
          1605,
          1552,
          1596,
          1572,
          1551,
          1605,
          1529,
          1600,
          1605,
          1598,
          1586,
          1561,
          1582,
          1601,
          1605,
          1601,
          1588,
          1446,
          1591,
          1605,
          1581,
          1606,
          1605,
          1605,
          1292,
          1601,
          1589,
          1604,
          1600,
          1605,
          1605,
          1597,
          1589,
          1581,
          1558,
          1604,
          1559,
          1580,
          1596,
          1559,
          1400,
          1597,
          1593,
          1606,
          1529,
          1601,
          1598,
          1363,
          1604,
          1604,
          1595,
          1568,
          1510,
          1603,
          1604,
          1605,
          1594,
          1605,
          1605,
          1599,
          1604,
          1573,
          1603,
          1291,
          1605,
          1605,
          1605,
          1605,
          1605,
          1605,
          1606,
          1597,
          1439,
          1604,
          978,
          1603,
          1605,
          1551,
          1605,
          1601,
          1586,
          1605,
          1315,
          1605,
          1594,
          1605,
          1597,
          1599,
          1448,
          1600,
          1596,
          1605,
          1571,
          1572,
          1594,
          1580,
          1586,
          1605,
          1592,
          1324,
          1598,
          1605,
          1586,
          1603,
          1567,
          1573,
          1589,
          1603,
          1601,
          1331,
          1557,
          1603,
          1602,
          1605,
          1598,
          1577,
          1605,
          1605,
          1605,
          1605,
          1604,
          1601,
          1598,
          1600,
          1600,
          1606,
          1363,
          1561,
          1604,
          1569,
          1595,
          1605,
          1579,
          1605,
          1604,
          1605,
          1580,
          1600,
          1575,
          1595,
          1572,
          1602,
          1555,
          1587,
          1241,
          1605,
          1546,
          1091,
          1593,
          1603,
          1577,
          1604,
          1595,
          1212,
          1605,
          1602,
          1605,
          1595,
          1573,
          1605,
          293,
          1533,
          1599,
          1605,
          1605,
          1606,
          1605,
          1605,
          1117,
          1542,
          1473,
          1187,
          1471,
          1582,
          1542,
          1605,
          1246,
          1597,
          1605,
          1605,
          1601,
          1588,
          1499,
          1576,
          1428,
          1603,
          1540,
          1605,
          1599,
          1605,
          1602,
          1585,
          1229,
          1604,
          1505,
          1605,
          1605,
          1599,
          1598,
          1580,
          1603,
          1594,
          1578,
          1602,
          1598,
          1601,
          1569,
          1605,
          1605,
          1598,
          1605,
          1599,
          1605,
          1605,
          1604,
          1606,
          1605,
          1605,
          1437,
          1602,
          1599,
          1603,
          1583,
          1605,
          1588,
          1536,
          1604,
          1604,
          1603,
          1588,
          1595,
          1603,
          1539,
          1587,
          1604,
          1606,
          1571,
          1593,
          1594
        ]
      },
      "prepare_train_sample": {
        "time": 0.10595321655273438
      },
      "train_eval_model": {
        "train_intermediate": [
          {
            "epoch": 0,
            "loss": 352.35009653618863,
            "eval_loss": 348.8451966785249,
            "train_recall": 0.004020151388572441,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 1,
            "loss": 352.31894233403284,
            "eval_loss": 348.8303808787512,
            "train_recall": 0.004032809295967191,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 2,
            "loss": 352.3134029661259,
            "eval_loss": 348.9602569095672,
            "train_recall": 0.004020151388572441,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 3,
            "loss": 352.32158648683867,
            "eval_loss": 348.87945387098523,
            "train_recall": 0.004032809295967191,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 4,
            "loss": 352.31249954177906,
            "eval_loss": 348.8526181417798,
            "train_recall": 0.004017619807093491,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 5,
            "loss": 352.3165551725864,
            "eval_loss": 348.8987517583938,
            "train_recall": 0.004032809295967191,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 6,
            "loss": 352.31548454856903,
            "eval_loss": 348.89683108859595,
            "train_recall": 0.004032809295967191,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 7,
            "loss": 352.31703350753696,
            "eval_loss": 348.79201156374006,
            "train_recall": 0.004032809295967191,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 8,
            "loss": 352.3198126476664,
            "eval_loss": 348.84876541864304,
            "train_recall": 0.00404546720336194,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 9,
            "loss": 352.3191990859308,
            "eval_loss": 348.9514110504635,
            "train_recall": 0.004032809295967191,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 10,
            "loss": 352.18222461665215,
            "eval_loss": 348.6913826352074,
            "train_recall": 0.004032809295967191,
            "val_recall": 0.0030075187969924814,
            "lr": 0.00168
          },
          {
            "epoch": 11,
            "loss": 352.1712059892018,
            "eval_loss": 348.70098780071925,
            "train_recall": 0.004032809295967191,
            "val_recall": 0.0030075187969924814,
            "lr": 0.00168
          }
        ],
        "train_total": {
          "correct": 1593,
          "final_recall": 0.004032809295967191
        },
        "training_time": 6248.189268350601,
        "eval_time": 0.2922191619873047
      }
    }
  ],
  "integrate": {
    "time": 50.8303108215332
  },
  "count_recall": {
    "time": 52.010472536087036
  }
}