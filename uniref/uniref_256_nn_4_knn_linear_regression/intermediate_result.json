{
  "total_time_consume": 25344.79380249977,
  "preprocess": {
    "total_time": 0.5423223972320557
  },
  "classifier": [
    {
      "ins_id": 0,
      "dataset_partition": {
        "bulid_graph_time": 15.58792781829834,
        "save_graph_time": 10.942623138427734,
        "graph_partition_time": 42.85748910903931,
        "total_time": 69.44986128807068,
        "cluster_number_distribution": [
          1606,
          1605,
          1605,
          1599,
          1605,
          1605,
          1604,
          1605,
          1600,
          1275,
          1605,
          1322,
          1605,
          1597,
          1598,
          1605,
          1531,
          1191,
          1602,
          1168,
          1603,
          1601,
          1602,
          1605,
          1589,
          1605,
          1039,
          1341,
          1605,
          1605,
          1605,
          1594,
          1605,
          1580,
          1603,
          1485,
          1562,
          1605,
          1589,
          1600,
          1600,
          1602,
          1605,
          1599,
          1605,
          1521,
          1465,
          1369,
          1597,
          1556,
          1561,
          1605,
          1597,
          1602,
          1175,
          1601,
          1604,
          1605,
          1297,
          1406,
          1605,
          1566,
          1547,
          1344,
          1603,
          1270,
          1605,
          1597,
          1605,
          1593,
          1605,
          1603,
          1605,
          1188,
          1605,
          1195,
          1569,
          921,
          1600,
          1596,
          1583,
          1590,
          1602,
          1583,
          1593,
          1605,
          1580,
          1604,
          1599,
          1383,
          1596,
          1603,
          1567,
          1546,
          1605,
          1307,
          1590,
          1600,
          1599,
          1605,
          1603,
          1546,
          1602,
          1590,
          1602,
          1572,
          1365,
          1339,
          1606,
          1604,
          1604,
          1602,
          1601,
          1540,
          1604,
          1538,
          1604,
          1574,
          1601,
          1580,
          1605,
          1589,
          1595,
          1605,
          1605,
          1602,
          1596,
          1601,
          1605,
          1603,
          1603,
          1594,
          1600,
          1483,
          1559,
          1586,
          1600,
          1472,
          1605,
          1598,
          1605,
          1601,
          1587,
          1411,
          1605,
          1265,
          1603,
          1585,
          1363,
          1603,
          1605,
          1605,
          1604,
          1336,
          1603,
          1521,
          1602,
          1560,
          1601,
          1489,
          1599,
          1604,
          1589,
          1603,
          1592,
          1603,
          1604,
          1603,
          1597,
          1602,
          1605,
          1605,
          1605,
          1592,
          1605,
          1604,
          1605,
          1604,
          1604,
          1601,
          1545,
          1602,
          1588,
          1604,
          1605,
          1604,
          1601,
          1591,
          1605,
          1601,
          1601,
          1604,
          1601,
          1533,
          1605,
          1600,
          1284,
          1599,
          1595,
          1601,
          1590,
          1602,
          1604,
          1604,
          1604,
          1598,
          1604,
          1589,
          1604,
          1601,
          1602,
          1602,
          1605,
          1604,
          1605,
          1602,
          1604,
          1601,
          1602,
          1456,
          1605,
          1605,
          1602,
          1605,
          1601,
          1579,
          1547,
          1325,
          1603,
          1593,
          1605,
          1404,
          1573,
          1605,
          1579,
          1601,
          1606,
          1533,
          1575,
          1604,
          1551,
          1346,
          1558,
          1547,
          1580,
          1603,
          1590,
          1605,
          1597,
          1599,
          1584,
          1580,
          1597,
          1605,
          1602,
          1601
        ]
      },
      "prepare_train_sample": {
        "time": 0.10707926750183105
      },
      "train_eval_model": {
        "train_intermediate": [
          {
            "epoch": 0,
            "loss": 352.24415631404713,
            "eval_loss": 348.63275764102025,
            "train_recall": 0.004025214551530341,
            "val_recall": 0.0040100250626566416,
            "lr": 0.008
          },
          {
            "epoch": 1,
            "loss": 352.2095527403285,
            "eval_loss": 348.69900391593814,
            "train_recall": 0.0040074934811776914,
            "val_recall": 0.0040100250626566416,
            "lr": 0.008
          },
          {
            "epoch": 2,
            "loss": 352.20725325388673,
            "eval_loss": 348.7638296702551,
            "train_recall": 0.004015088225614542,
            "val_recall": 0.0040100250626566416,
            "lr": 0.008
          },
          {
            "epoch": 3,
            "loss": 352.2141265455104,
            "eval_loss": 348.6258477105035,
            "train_recall": 0.004025214551530341,
            "val_recall": 0.0040100250626566416,
            "lr": 0.008
          },
          {
            "epoch": 4,
            "loss": 352.20411613287615,
            "eval_loss": 348.60882386707124,
            "train_recall": 0.004022682970051391,
            "val_recall": 0.0040100250626566416,
            "lr": 0.008
          },
          {
            "epoch": 5,
            "loss": 352.20954302854676,
            "eval_loss": 348.6636147877527,
            "train_recall": 0.004022682970051391,
            "val_recall": 0.0040100250626566416,
            "lr": 0.008
          },
          {
            "epoch": 6,
            "loss": 352.20696473395867,
            "eval_loss": 348.7058772738018,
            "train_recall": 0.00404040404040404,
            "val_recall": 0.0040100250626566416,
            "lr": 0.008
          },
          {
            "epoch": 7,
            "loss": 352.2096689696368,
            "eval_loss": 348.7058618939112,
            "train_recall": 0.004022682970051391,
            "val_recall": 0.0040100250626566416,
            "lr": 0.008
          },
          {
            "epoch": 8,
            "loss": 352.21891129408897,
            "eval_loss": 348.5946261693561,
            "train_recall": 0.004012556644135592,
            "val_recall": 0.0040100250626566416,
            "lr": 0.008
          },
          {
            "epoch": 9,
            "loss": 352.2112778094026,
            "eval_loss": 348.7503742036365,
            "train_recall": 0.004022682970051391,
            "val_recall": 0.0040100250626566416,
            "lr": 0.008
          },
          {
            "epoch": 10,
            "loss": 352.0758442306333,
            "eval_loss": 348.51799144442117,
            "train_recall": 0.004022682970051391,
            "val_recall": 0.0040100250626566416,
            "lr": 0.00168
          },
          {
            "epoch": 11,
            "loss": 352.06711207095833,
            "eval_loss": 348.49416993156314,
            "train_recall": 0.004022682970051391,
            "val_recall": 0.0040100250626566416,
            "lr": 0.00168
          }
        ],
        "train_total": {
          "correct": 1589,
          "final_recall": 0.004022682970051391
        },
        "training_time": 6191.904620885849,
        "eval_time": 0.2846686840057373
      }
    },
    {
      "ins_id": 1,
      "dataset_partition": {
        "bulid_graph_time": 13.094033241271973,
        "save_graph_time": 10.438724756240845,
        "graph_partition_time": 50.742942810058594,
        "total_time": 74.33577156066895,
        "cluster_number_distribution": [
          1603,
          1605,
          1596,
          1597,
          1601,
          1602,
          1603,
          1605,
          1605,
          1602,
          1605,
          1597,
          1603,
          1605,
          1599,
          1605,
          1603,
          1597,
          1603,
          1605,
          1593,
          1604,
          1605,
          1603,
          1600,
          1605,
          1603,
          1605,
          1605,
          1600,
          1605,
          1605,
          1598,
          1605,
          1605,
          1585,
          1604,
          1576,
          1605,
          1590,
          1605,
          1605,
          1605,
          1600,
          1602,
          1602,
          1591,
          1602,
          1601,
          1604,
          1603,
          1601,
          1603,
          1574,
          1592,
          1602,
          1605,
          1604,
          1602,
          1592,
          1602,
          1604,
          1602,
          1605,
          1589,
          1602,
          1604,
          1605,
          1605,
          1600,
          1531,
          1600,
          1599,
          1603,
          1605,
          1601,
          1600,
          1586,
          1605,
          1603,
          1592,
          1602,
          1605,
          1602,
          1605,
          1572,
          1602,
          1591,
          1338,
          1596,
          1593,
          1596,
          1602,
          1605,
          1598,
          1581,
          1605,
          1601,
          1602,
          1549,
          1605,
          1603,
          1605,
          1598,
          1605,
          1606,
          1603,
          1600,
          1596,
          1604,
          1605,
          1603,
          1604,
          1605,
          1605,
          1603,
          1604,
          1605,
          1605,
          1606,
          1602,
          1605,
          1584,
          1593,
          1603,
          1605,
          1605,
          1605,
          1603,
          1604,
          1565,
          1581,
          1604,
          1576,
          1602,
          1600,
          1596,
          1604,
          1604,
          1313,
          1606,
          1571,
          1604,
          1519,
          1589,
          1071,
          1604,
          1567,
          1603,
          498,
          1604,
          1523,
          1605,
          1593,
          1605,
          1603,
          1490,
          1606,
          1582,
          1604,
          1605,
          1547,
          1605,
          1594,
          1605,
          1568,
          1604,
          269,
          1605,
          1604,
          1605,
          835,
          1604,
          1508,
          1341,
          1603,
          1588,
          1591,
          1592,
          1574,
          1603,
          1605,
          1605,
          1602,
          1596,
          1580,
          1605,
          1581,
          1571,
          1566,
          1577,
          1566,
          1584,
          1573,
          1603,
          1595,
          1601,
          1552,
          1557,
          1527,
          1157,
          1543,
          1528,
          1588,
          1605,
          1599,
          1585,
          390,
          1605,
          556,
          1605,
          1605,
          1599,
          1595,
          1560,
          1606,
          1605,
          1605,
          1605,
          1605,
          1605,
          1341,
          1538,
          909,
          1603,
          1571,
          1603,
          1596,
          1605,
          1596,
          1587,
          1552,
          1605,
          1277,
          1605,
          1605,
          1589,
          1598,
          1567,
          1605,
          1585,
          1483,
          1605,
          1263,
          1513,
          1604,
          1602,
          1551,
          1602,
          1490,
          1603,
          1555,
          1602,
          1604,
          1604,
          1539
        ]
      },
      "prepare_train_sample": {
        "time": 0.12293362617492676
      },
      "train_eval_model": {
        "train_intermediate": [
          {
            "epoch": 0,
            "loss": 351.73979372215797,
            "eval_loss": 348.39579906160867,
            "train_recall": 0.004020151388572441,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 1,
            "loss": 351.6965242392353,
            "eval_loss": 348.31183539496527,
            "train_recall": 0.004027746133009291,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 2,
            "loss": 351.6966469522313,
            "eval_loss": 348.29139939565505,
            "train_recall": 0.00404040404040404,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 3,
            "loss": 351.69677494697316,
            "eval_loss": 348.35118066696896,
            "train_recall": 0.00403787245892509,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 4,
            "loss": 351.69595956269376,
            "eval_loss": 348.2470347086589,
            "train_recall": 0.004020151388572441,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 5,
            "loss": 351.6979095162979,
            "eval_loss": 348.26421598404175,
            "train_recall": 0.004032809295967191,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 6,
            "loss": 351.7005611789221,
            "eval_loss": 348.28436775813026,
            "train_recall": 0.004027746133009291,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 7,
            "loss": 351.696084576682,
            "eval_loss": 348.316902523949,
            "train_recall": 0.004030277714488241,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 8,
            "loss": 351.6962933987596,
            "eval_loss": 348.3293905106802,
            "train_recall": 0.004020151388572441,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 9,
            "loss": 351.69498483554946,
            "eval_loss": 348.3098878406343,
            "train_recall": 0.004083440925546189,
            "val_recall": 0.0030075187969924814,
            "lr": 0.008
          },
          {
            "epoch": 10,
            "loss": 351.58965963493114,
            "eval_loss": 348.2238093784877,
            "train_recall": 0.004032809295967191,
            "val_recall": 0.0030075187969924814,
            "lr": 0.00168
          },
          {
            "epoch": 11,
            "loss": 351.5840253301596,
            "eval_loss": 348.2054404606895,
            "train_recall": 0.004032809295967191,
            "val_recall": 0.0030075187969924814,
            "lr": 0.00168
          }
        ],
        "train_total": {
          "correct": 1593,
          "final_recall": 0.004032809295967191
        },
        "training_time": 6205.3780682086945,
        "eval_time": 0.2813293933868408
      }
    },
    {
      "ins_id": 2,
      "dataset_partition": {
        "bulid_graph_time": 16.383870363235474,
        "save_graph_time": 10.180602073669434,
        "graph_partition_time": 51.93513345718384,
        "total_time": 78.56112575531006,
        "cluster_number_distribution": [
          1547,
          1586,
          1420,
          1549,
          1544,
          1604,
          1564,
          1604,
          1552,
          1590,
          1599,
          1493,
          1602,
          1467,
          1589,
          1601,
          1580,
          1604,
          1605,
          1603,
          1569,
          1602,
          1605,
          1599,
          1605,
          1556,
          1605,
          1358,
          1604,
          1598,
          1605,
          1590,
          1597,
          1583,
          1605,
          1553,
          1605,
          742,
          1605,
          1600,
          1602,
          1568,
          1595,
          1582,
          1602,
          1605,
          1604,
          1523,
          1602,
          1567,
          1602,
          1393,
          1605,
          1593,
          1560,
          1588,
          1603,
          1605,
          1565,
          1577,
          1599,
          1605,
          1605,
          1606,
          1605,
          1602,
          1602,
          1210,
          1571,
          1604,
          1601,
          1604,
          1537,
          1605,
          1597,
          1599,
          1599,
          1600,
          1603,
          1603,
          1602,
          1353,
          1601,
          1598,
          1605,
          1468,
          1605,
          1604,
          1600,
          1603,
          1597,
          1595,
          1605,
          1595,
          1604,
          1600,
          1603,
          1605,
          1606,
          1099,
          1558,
          1601,
          1540,
          1380,
          1604,
          1605,
          1570,
          1604,
          1604,
          1570,
          1597,
          1603,
          1606,
          1494,
          1471,
          1595,
          1602,
          1602,
          1596,
          1567,
          1605,
          887,
          1142,
          559,
          1604,
          1605,
          1605,
          1605,
          1605,
          1605,
          1605,
          1604,
          388,
          1604,
          1605,
          1605,
          1497,
          1397,
          1603,
          1164,
          1592,
          1597,
          1091,
          1604,
          1529,
          1604,
          1592,
          1605,
          1592,
          1579,
          1602,
          1587,
          1537,
          1605,
          1571,
          1600,
          1605,
          1236,
          1605,
          1584,
          1603,
          1599,
          1605,
          1596,
          1604,
          1592,
          1596,
          1601,
          1605,
          1605,
          1605,
          1592,
          1605,
          1604,
          1603,
          1604,
          1604,
          1599,
          1605,
          1598,
          1597,
          1602,
          1605,
          1605,
          1605,
          1605,
          1605,
          1602,
          1602,
          1583,
          1605,
          1602,
          1604,
          1604,
          1569,
          1602,
          1572,
          1604,
          1605,
          1547,
          1604,
          1117,
          1563,
          1604,
          1604,
          1605,
          1605,
          1605,
          1598,
          1600,
          1560,
          1601,
          1604,
          1604,
          1602,
          1605,
          1605,
          1592,
          1598,
          1595,
          1605,
          1605,
          1603,
          1355,
          1603,
          1594,
          1604,
          1593,
          1605,
          1603,
          1603,
          1598,
          1604,
          1605,
          1605,
          1538,
          1593,
          1600,
          1605,
          1604,
          1496,
          1604,
          1605,
          1604,
          1596,
          1595,
          1597,
          1601,
          1605,
          1589,
          1604,
          1605,
          1604,
          1598,
          1603,
          1433
        ]
      },
      "prepare_train_sample": {
        "time": 0.11866497993469238
      },
      "train_eval_model": {
        "train_intermediate": [
          {
            "epoch": 0,
            "loss": 352.50207143144286,
            "eval_loss": 348.89095100523934,
            "train_recall": 0.004103693577377788,
            "val_recall": 0.005513784461152882,
            "lr": 0.008
          },
          {
            "epoch": 1,
            "loss": 352.4687637524996,
            "eval_loss": 348.84217362176804,
            "train_recall": 0.004004961899698742,
            "val_recall": 0.005513784461152882,
            "lr": 0.008
          },
          {
            "epoch": 2,
            "loss": 352.46671438781055,
            "eval_loss": 348.8725817241366,
            "train_recall": 0.004055593529277739,
            "val_recall": 0.005513784461152882,
            "lr": 0.008
          },
          {
            "epoch": 3,
            "loss": 352.4669719744246,
            "eval_loss": 348.92760770283047,
            "train_recall": 0.003934077618288145,
            "val_recall": 0.002756892230576441,
            "lr": 0.008
          },
          {
            "epoch": 4,
            "loss": 352.4747538125517,
            "eval_loss": 348.873537336077,
            "train_recall": 0.00406318827371459,
            "val_recall": 0.005513784461152882,
            "lr": 0.008
          },
          {
            "epoch": 5,
            "loss": 352.4742810629223,
            "eval_loss": 348.8850562686012,
            "train_recall": 0.004068251436672489,
            "val_recall": 0.005513784461152882,
            "lr": 0.008
          },
          {
            "epoch": 6,
            "loss": 352.4698845371906,
            "eval_loss": 348.815054636153,
            "train_recall": 0.004012556644135592,
            "val_recall": 0.005513784461152882,
            "lr": 0.008
          },
          {
            "epoch": 7,
            "loss": 352.473366310352,
            "eval_loss": 348.8874879867312,
            "train_recall": 0.003999898736740842,
            "val_recall": 0.005513784461152882,
            "lr": 0.008
          },
          {
            "epoch": 8,
            "loss": 352.4687430673609,
            "eval_loss": 348.8783535427517,
            "train_recall": 0.003941672362724994,
            "val_recall": 0.002756892230576441,
            "lr": 0.008
          },
          {
            "epoch": 9,
            "loss": 352.46981109453526,
            "eval_loss": 348.93745482914034,
            "train_recall": 0.0039923039923039924,
            "val_recall": 0.005513784461152882,
            "lr": 0.008
          },
          {
            "epoch": 10,
            "loss": 352.34502403473095,
            "eval_loss": 348.77191767616875,
            "train_recall": 0.0040074934811776914,
            "val_recall": 0.005513784461152882,
            "lr": 0.00168
          },
          {
            "epoch": 11,
            "loss": 352.33198467599465,
            "eval_loss": 348.7732548789373,
            "train_recall": 0.0040074934811776914,
            "val_recall": 0.005513784461152882,
            "lr": 0.00168
          }
        ],
        "train_total": {
          "correct": 1583,
          "final_recall": 0.0040074934811776914
        },
        "training_time": 6207.42041015625,
        "eval_time": 0.28430676460266113
      }
    },
    {
      "ins_id": 3,
      "dataset_partition": {
        "bulid_graph_time": 15.020922660827637,
        "save_graph_time": 11.694828748703003,
        "graph_partition_time": 59.04771661758423,
        "total_time": 85.82462573051453,
        "cluster_number_distribution": [
          1606,
          1606,
          1604,
          1606,
          1608,
          1606,
          1605,
          1607,
          1606,
          1295,
          1607,
          961,
          1607,
          1382,
          1605,
          1605,
          1608,
          1605,
          1609,
          1605,
          1605,
          1541,
          1529,
          1546,
          1606,
          1609,
          1605,
          1609,
          1573,
          1614,
          1605,
          1605,
          1608,
          1606,
          1605,
          1605,
          1605,
          1549,
          1605,
          971,
          1606,
          1604,
          1605,
          1605,
          1606,
          1607,
          1605,
          1605,
          1605,
          1606,
          1404,
          1606,
          1605,
          1605,
          1607,
          1611,
          1605,
          1605,
          1605,
          1605,
          1605,
          1606,
          1605,
          1605,
          1606,
          1608,
          1604,
          1605,
          1605,
          1604,
          1607,
          1608,
          1607,
          633,
          1605,
          1606,
          1607,
          1607,
          1611,
          1607,
          1605,
          1607,
          1609,
          1605,
          1606,
          1604,
          1604,
          1607,
          1605,
          1607,
          1605,
          1606,
          1605,
          1605,
          1607,
          1526,
          1547,
          1605,
          1605,
          1605,
          1605,
          1603,
          1605,
          1605,
          1607,
          1608,
          1605,
          1539,
          1608,
          1605,
          1605,
          1604,
          1607,
          1605,
          1610,
          1612,
          1605,
          1605,
          1604,
          1026,
          1606,
          1496,
          1605,
          850,
          1604,
          1553,
          1424,
          1607,
          1605,
          1607,
          1607,
          1605,
          1608,
          1605,
          1605,
          1376,
          1607,
          1566,
          1582,
          1422,
          1609,
          1396,
          1608,
          1548,
          1605,
          1524,
          1605,
          1605,
          1605,
          1603,
          1605,
          1365,
          1605,
          1607,
          1605,
          1605,
          1605,
          1607,
          1604,
          1544,
          1608,
          1574,
          1527,
          1528,
          1605,
          1604,
          1569,
          1605,
          1605,
          1605,
          1574,
          1514,
          1605,
          1543,
          1412,
          1149,
          1606,
          1605,
          1607,
          612,
          1605,
          1606,
          1606,
          1609,
          1608,
          1605,
          1605,
          1606,
          1605,
          1605,
          1605,
          1602,
          1238,
          1605,
          1608,
          1600,
          1603,
          1608,
          1605,
          1606,
          1605,
          1606,
          1605,
          1605,
          1605,
          1131,
          1607,
          1276,
          1607,
          1606,
          1610,
          1608,
          1605,
          1307,
          1607,
          1606,
          1613,
          1605,
          712,
          1610,
          1605,
          735,
          1608,
          1607,
          1605,
          1606,
          1607,
          1538,
          1606,
          1605,
          1608,
          1573,
          1555,
          1606,
          1405,
          1605,
          1605,
          1606,
          1606,
          1604,
          1605,
          1476,
          1608,
          1605,
          1606,
          1606,
          1604,
          1592,
          1595,
          1608,
          1582,
          1605,
          1607,
          1608,
          1606,
          1604
        ]
      },
      "prepare_train_sample": {
        "time": 0.11576151847839355
      },
      "train_eval_model": {
        "train_intermediate": [
          {
            "epoch": 0,
            "loss": 352.6729889574148,
            "eval_loss": 349.3094800918821,
            "train_recall": 0.0039442039442039445,
            "val_recall": 0.006265664160401002,
            "lr": 0.008
          },
          {
            "epoch": 1,
            "loss": 352.65004220079146,
            "eval_loss": 349.1546659923735,
            "train_recall": 0.004035340877446141,
            "val_recall": 0.006265664160401002,
            "lr": 0.008
          },
          {
            "epoch": 2,
            "loss": 352.65395534712513,
            "eval_loss": 349.2482614668589,
            "train_recall": 0.004197362092098934,
            "val_recall": 0.006265664160401002,
            "lr": 0.008
          },
          {
            "epoch": 3,
            "loss": 352.651142918154,
            "eval_loss": 349.28756459554035,
            "train_recall": 0.004085972507025138,
            "val_recall": 0.0035087719298245615,
            "lr": 0.008
          },
          {
            "epoch": 4,
            "loss": 352.6517246948655,
            "eval_loss": 349.2727365645151,
            "train_recall": 0.004002430318219792,
            "val_recall": 0.003258145363408521,
            "lr": 0.008
          },
          {
            "epoch": 5,
            "loss": 352.6457580453319,
            "eval_loss": 349.28372592017763,
            "train_recall": 0.00404799878484089,
            "val_recall": 0.0035087719298245615,
            "lr": 0.008
          },
          {
            "epoch": 6,
            "loss": 352.6471690808705,
            "eval_loss": 349.1597027248806,
            "train_recall": 0.003936609199767094,
            "val_recall": 0.0035087719298245615,
            "lr": 0.008
          },
          {
            "epoch": 7,
            "loss": 352.65310171808005,
            "eval_loss": 349.24288189600384,
            "train_recall": 0.003964456596035544,
            "val_recall": 0.003258145363408521,
            "lr": 0.008
          },
          {
            "epoch": 8,
            "loss": 352.64720391755975,
            "eval_loss": 349.335322183276,
            "train_recall": 0.0040581251107566895,
            "val_recall": 0.003258145363408521,
            "lr": 0.008
          },
          {
            "epoch": 9,
            "loss": 352.64958987814595,
            "eval_loss": 349.38436441572884,
            "train_recall": 0.004149262043998886,
            "val_recall": 0.003258145363408521,
            "lr": 0.008
          },
          {
            "epoch": 10,
            "loss": 352.5249670265985,
            "eval_loss": 349.0992632669116,
            "train_recall": 0.004027746133009291,
            "val_recall": 0.003258145363408521,
            "lr": 0.00168
          },
          {
            "epoch": 11,
            "loss": 352.5154130846991,
            "eval_loss": 349.10612039717415,
            "train_recall": 0.004020151388572441,
            "val_recall": 0.003258145363408521,
            "lr": 0.00168
          }
        ],
        "train_total": {
          "correct": 1588,
          "final_recall": 0.004020151388572441
        },
        "training_time": 6203.804649591446,
        "eval_time": 0.28310489654541016
      }
    }
  ],
  "integrate": {
    "time": 139.46081495285034
  },
  "count_recall": {
    "time": 64.79902124404907
  }
}